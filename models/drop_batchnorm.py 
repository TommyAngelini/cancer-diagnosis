#BatchNormalization on all layers, drop out = 0.50, reg_param = 0.0015

#define regularization and droprate hyperparameters
reg_param = 0.0015
droprate = 0.50

#build model
model = Sequential()

#convolution 1, with batch norm
model.add(Conv2D(filters=64,strides=(2, 2), use_bias=False, kernel_size=5,
                                 padding='same',
                                 kernel_regularizer=tf.keras.regularizers.l2(reg_param),
                                 input_shape=(50,50,4)))

model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(droprate))


#convolution 2, with batch norm and max pooling
model.add(Conv2D(filters=128,strides=(2, 2), use_bias=False, kernel_size=5,
                                 padding='same',
                                 kernel_regularizer=tf.keras.regularizers.l2(reg_param)))

model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(droprate))

#convolution 3, with batch norm and max pooling
model.add(Conv2D(filters=256,strides=(2, 2), use_bias=False, kernel_size=3,
                                 padding='same',
                                 kernel_regularizer=tf.keras.regularizers.l2(reg_param)))

model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(droprate))


#dense layer
model.add(Flatten())
model.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(reg_param)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(droprate))

model.add(Dense(2, activation='softmax'))

model.summary()
